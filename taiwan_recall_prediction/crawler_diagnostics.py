#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬èŸ²è¨ºæ–·å·¥å…·
Crawler Diagnostics Tool

è¨ºæ–·PTTã€Dcardç­‰çˆ¬èŸ²ä¸å¯ç”¨çš„å…·é«”åŸå› ä¸¦æä¾›è§£æ±ºæ–¹æ¡ˆ
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CrawlerDiagnostics:
    """çˆ¬èŸ²è¨ºæ–·é¡"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        self.test_results = {}
    
    def diagnose_ptt_crawler(self, candidate_name="ç¾…æ™ºå¼·"):
        """è¨ºæ–·PTTçˆ¬èŸ²å•é¡Œ"""
        print("ğŸ” è¨ºæ–·PTTçˆ¬èŸ²...")
        
        try:
            # æ¸¬è©¦PTTç¶²ç«™é€£æ¥
            ptt_url = "https://www.ptt.cc/"
            response = requests.get(ptt_url, headers=self.headers, timeout=10)
            
            print(f"PTTä¸»ç«™é€£æ¥ç‹€æ…‹: {response.status_code}")
            
            if response.status_code == 200:
                print("âœ… PTTä¸»ç«™å¯ä»¥æ­£å¸¸è¨ªå•")
                
                # æ¸¬è©¦æœå°‹åŠŸèƒ½
                search_url = f"https://www.ptt.cc/bbs/search?q={candidate_name}"
                search_response = requests.get(search_url, headers=self.headers, timeout=10)
                
                print(f"PTTæœå°‹é é¢ç‹€æ…‹: {search_response.status_code}")
                
                if search_response.status_code == 200:
                    soup = BeautifulSoup(search_response.text, 'html.parser')
                    
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦å¹´é½¡é©—è­‰
                    if "over18" in search_response.text or "æ»¿18æ­²" in search_response.text:
                        print("âš ï¸ PTTéœ€è¦å¹´é½¡é©—è­‰ - é€™æ˜¯ä¸»è¦å•é¡Œï¼")
                        print("è§£æ±ºæ–¹æ¡ˆ: éœ€è¦å…ˆé€šéå¹´é½¡é©—è­‰é é¢")
                        return self._test_ptt_with_age_verification(candidate_name)
                    
                    # æª¢æŸ¥æœå°‹çµæœ
                    posts = soup.find_all('div', class_='r-ent')
                    print(f"æ‰¾åˆ°æ–‡ç« æ•¸é‡: {len(posts)}")
                    
                    if len(posts) > 0:
                        print("âœ… PTTæœå°‹åŠŸèƒ½æ­£å¸¸")
                        return {
                            'status': 'success',
                            'posts_found': len(posts),
                            'issue': None
                        }
                    else:
                        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡ç« ")
                        return {
                            'status': 'no_data',
                            'posts_found': 0,
                            'issue': 'æœå°‹çµæœç‚ºç©º'
                        }
                else:
                    print(f"âŒ PTTæœå°‹é é¢ç„¡æ³•è¨ªå•: {search_response.status_code}")
                    return {
                        'status': 'search_failed',
                        'posts_found': 0,
                        'issue': f'æœå°‹é é¢HTTP {search_response.status_code}'
                    }
            else:
                print(f"âŒ PTTä¸»ç«™ç„¡æ³•è¨ªå•: {response.status_code}")
                return {
                    'status': 'connection_failed',
                    'posts_found': 0,
                    'issue': f'ä¸»ç«™HTTP {response.status_code}'
                }
                
        except Exception as e:
            print(f"âŒ PTTçˆ¬èŸ²éŒ¯èª¤: {e}")
            return {
                'status': 'error',
                'posts_found': 0,
                'issue': str(e)
            }
    
    def _test_ptt_with_age_verification(self, candidate_name):
        """æ¸¬è©¦PTTå¹´é½¡é©—è­‰è§£æ±ºæ–¹æ¡ˆ"""
        print("ğŸ”§ å˜—è©¦è§£æ±ºPTTå¹´é½¡é©—è­‰å•é¡Œ...")
        
        try:
            # å‰µå»ºsessionä¾†ä¿æŒcookies
            session = requests.Session()
            session.headers.update(self.headers)
            
            # å…ˆè¨ªå•å¹´é½¡é©—è­‰é é¢
            over18_url = "https://www.ptt.cc/ask/over18"
            response = session.get(over18_url, timeout=10)
            
            if response.status_code == 200:
                # æäº¤å¹´é½¡é©—è­‰
                verify_data = {
                    'from': '/bbs/Gossiping/index.html',
                    'yes': 'yes'
                }
                
                verify_response = session.post(over18_url, data=verify_data, timeout=10)
                
                if verify_response.status_code == 200:
                    print("âœ… å¹´é½¡é©—è­‰é€šé")
                    
                    # ç¾åœ¨å˜—è©¦æœå°‹
                    search_url = f"https://www.ptt.cc/bbs/search?q={candidate_name}"
                    search_response = session.get(search_url, timeout=10)
                    
                    if search_response.status_code == 200:
                        soup = BeautifulSoup(search_response.text, 'html.parser')
                        posts = soup.find_all('div', class_='r-ent')
                        
                        print(f"âœ… å¹´é½¡é©—è­‰å¾Œæ‰¾åˆ°æ–‡ç« æ•¸é‡: {len(posts)}")
                        
                        return {
                            'status': 'success_with_verification',
                            'posts_found': len(posts),
                            'issue': None,
                            'solution': 'éœ€è¦å¹´é½¡é©—è­‰'
                        }
                    
        except Exception as e:
            print(f"âŒ å¹´é½¡é©—è­‰è§£æ±ºæ–¹æ¡ˆå¤±æ•—: {e}")
        
        return {
            'status': 'verification_failed',
            'posts_found': 0,
            'issue': 'å¹´é½¡é©—è­‰å¤±æ•—',
            'solution': 'éœ€è¦æ‰‹å‹•è™•ç†å¹´é½¡é©—è­‰'
        }
    
    def diagnose_dcard_crawler(self, candidate_name="ç¾…æ™ºå¼·"):
        """è¨ºæ–·Dcardçˆ¬èŸ²å•é¡Œ"""
        print("ğŸ” è¨ºæ–·Dcardçˆ¬èŸ²...")
        
        try:
            # æ¸¬è©¦Dcard API
            api_url = "https://www.dcard.tw/service/api/v2/posts/search"
            
            params = {
                'query': candidate_name,
                'limit': 10
            }
            
            response = requests.get(api_url, headers=self.headers, params=params, timeout=10)
            
            print(f"Dcard APIç‹€æ…‹: {response.status_code}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"âœ… Dcard APIæ­£å¸¸ï¼Œæ‰¾åˆ°æ–‡ç« æ•¸é‡: {len(data)}")
                    
                    return {
                        'status': 'success',
                        'posts_found': len(data),
                        'issue': None
                    }
                    
                except json.JSONDecodeError:
                    print("âš ï¸ Dcard APIè¿”å›éJSONæ ¼å¼")
                    return {
                        'status': 'json_error',
                        'posts_found': 0,
                        'issue': 'APIè¿”å›æ ¼å¼éŒ¯èª¤'
                    }
            
            elif response.status_code == 429:
                print("âš ï¸ Dcard APIè«‹æ±‚é »ç‡é™åˆ¶")
                return {
                    'status': 'rate_limited',
                    'posts_found': 0,
                    'issue': 'APIè«‹æ±‚é »ç‡éé«˜',
                    'solution': 'éœ€è¦é™ä½è«‹æ±‚é »ç‡æˆ–ä½¿ç”¨ä»£ç†'
                }
            
            else:
                print(f"âŒ Dcard APIéŒ¯èª¤: {response.status_code}")
                return {
                    'status': 'api_error',
                    'posts_found': 0,
                    'issue': f'API HTTP {response.status_code}'
                }
                
        except Exception as e:
            print(f"âŒ Dcardçˆ¬èŸ²éŒ¯èª¤: {e}")
            return {
                'status': 'error',
                'posts_found': 0,
                'issue': str(e)
            }
    
    def diagnose_news_crawler(self, candidate_name="ç¾…æ™ºå¼·"):
        """è¨ºæ–·æ–°èçˆ¬èŸ²å•é¡Œ"""
        print("ğŸ” è¨ºæ–·æ–°èçˆ¬èŸ²...")
        
        news_sources = [
            ("è¯åˆæ–°èç¶²", f"https://udn.com/search/result/2/{candidate_name}"),
            ("ä¸­æ™‚æ–°èç¶²", f"https://www.chinatimes.com/search/{candidate_name}"),
            ("è‡ªç”±æ™‚å ±", f"https://search.ltn.com.tw/list?keyword={candidate_name}")
        ]
        
        results = {}
        
        for source_name, url in news_sources:
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ç°¡å–®æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹
                    if len(soup.text) > 1000:  # åŸºæœ¬å…§å®¹æª¢æŸ¥
                        print(f"âœ… {source_name} å¯ä»¥æ­£å¸¸è¨ªå•")
                        results[source_name] = {
                            'status': 'success',
                            'issue': None
                        }
                    else:
                        print(f"âš ï¸ {source_name} å…§å®¹ç•°å¸¸")
                        results[source_name] = {
                            'status': 'content_error',
                            'issue': 'é é¢å…§å®¹ç•°å¸¸'
                        }
                else:
                    print(f"âŒ {source_name} HTTPéŒ¯èª¤: {response.status_code}")
                    results[source_name] = {
                        'status': 'http_error',
                        'issue': f'HTTP {response.status_code}'
                    }
                    
            except Exception as e:
                print(f"âŒ {source_name} éŒ¯èª¤: {e}")
                results[source_name] = {
                    'status': 'error',
                    'issue': str(e)
                }
        
        return results
    
    def generate_diagnostic_report(self):
        """ç”Ÿæˆè¨ºæ–·å ±å‘Š"""
        print("\n" + "="*50)
        print("ğŸ“‹ çˆ¬èŸ²è¨ºæ–·å ±å‘Š")
        print("="*50)
        
        candidate_name = "ç¾…æ™ºå¼·"
        
        # è¨ºæ–·PTT
        ptt_result = self.diagnose_ptt_crawler(candidate_name)
        print(f"\nğŸ” PTTè¨ºæ–·çµæœ:")
        print(f"   ç‹€æ…‹: {ptt_result['status']}")
        print(f"   æ‰¾åˆ°æ–‡ç« : {ptt_result['posts_found']}")
        if ptt_result['issue']:
            print(f"   å•é¡Œ: {ptt_result['issue']}")
        if 'solution' in ptt_result:
            print(f"   è§£æ±ºæ–¹æ¡ˆ: {ptt_result['solution']}")
        
        # è¨ºæ–·Dcard
        dcard_result = self.diagnose_dcard_crawler(candidate_name)
        print(f"\nğŸ” Dcardè¨ºæ–·çµæœ:")
        print(f"   ç‹€æ…‹: {dcard_result['status']}")
        print(f"   æ‰¾åˆ°æ–‡ç« : {dcard_result['posts_found']}")
        if dcard_result['issue']:
            print(f"   å•é¡Œ: {dcard_result['issue']}")
        if 'solution' in dcard_result:
            print(f"   è§£æ±ºæ–¹æ¡ˆ: {dcard_result['solution']}")
        
        # è¨ºæ–·æ–°è
        news_results = self.diagnose_news_crawler(candidate_name)
        print(f"\nğŸ” æ–°èçˆ¬èŸ²è¨ºæ–·çµæœ:")
        for source, result in news_results.items():
            print(f"   {source}: {result['status']}")
            if result['issue']:
                print(f"      å•é¡Œ: {result['issue']}")
        
        # ç¸½çµå’Œå»ºè­°
        print(f"\nğŸ’¡ ç¸½çµå’Œå»ºè­°:")
        
        if ptt_result['status'] in ['success', 'success_with_verification']:
            print("   âœ… PTTçˆ¬èŸ²å¯ä»¥ä¿®å¾©")
        else:
            print("   âŒ PTTçˆ¬èŸ²éœ€è¦é€²ä¸€æ­¥è™•ç†")
        
        if dcard_result['status'] == 'success':
            print("   âœ… Dcardçˆ¬èŸ²æ­£å¸¸")
        else:
            print("   âŒ Dcardçˆ¬èŸ²éœ€è¦ä¿®å¾©")
        
        working_news = sum(1 for result in news_results.values() if result['status'] == 'success')
        print(f"   ğŸ“° æ–°èçˆ¬èŸ²: {working_news}/{len(news_results)} å€‹ä¾†æºæ­£å¸¸")
        
        return {
            'ptt': ptt_result,
            'dcard': dcard_result,
            'news': news_results,
            'timestamp': datetime.now().isoformat()
        }

if __name__ == "__main__":
    diagnostics = CrawlerDiagnostics()
    report = diagnostics.generate_diagnostic_report()
    
    # ä¿å­˜è¨ºæ–·å ±å‘Š
    with open('crawler_diagnostic_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¨ºæ–·å ±å‘Šå·²ä¿å­˜åˆ°: crawler_diagnostic_report.json")

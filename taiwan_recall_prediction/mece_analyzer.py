#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MECEåˆ†ææ¨¡çµ„ - äº’æ–¥ä¸”å®Œå…¨çª®ç›¡çš„åˆ†ææ¡†æ¶
åŸºæ–¼ChatGPTå»ºè­°çš„å®Œæ•´MECEæ¡†æ¶å„ªåŒ–ï¼š
1. æŠ•ç¥¨ç‡é æ¸¬ (çµæ§‹æ€§å› ç´ ã€å‹•æ©Ÿå› ç´ ã€ç¤¾ç¾¤åª’é«”è²é‡)
2. æŠ•ç¥¨çµæœé æ¸¬ (æ³•è¦é–€æª»ã€é¸æ°‘çµæ§‹ã€æƒ…ç·’äº‹ä»¶è®Šæ•¸)
3. 7ç¶­åº¦é‡åŒ–æŒ‡æ¨™ç³»çµ±
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import jieba
from collections import Counter
import re
import requests
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, mean_squared_error
import joblib
import glob
import warnings
warnings.filterwarnings('ignore')

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class MECEAnalyzer:
    def __init__(self):
        self.demographic_keywords = self._load_demographic_keywords()
        self.issue_keywords = self._load_issue_keywords()
        self.quantitative_indicators = self._init_quantitative_indicators()
        self.recall_threshold = 0.25  # 25%åŒæ„é–€æª»

    def _load_optimized_model(self):
        """è¼‰å…¥å„ªåŒ–å¾Œçš„æ¨¡å‹"""
        try:
            # å°‹æ‰¾æœ€æ–°çš„å„ªåŒ–æ¨¡å‹æ–‡ä»¶
            model_files = glob.glob("optimized_model_*.joblib")
            if not model_files:
                print("æœªæ‰¾åˆ°å„ªåŒ–æ¨¡å‹æ–‡ä»¶")
                return None

            # é¸æ“‡æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            latest_model_file = max(model_files, key=lambda x: x.split('_')[-1])

            # è¼‰å…¥æ¨¡å‹åŒ…
            model_package = joblib.load(latest_model_file)

            print(f"æˆåŠŸè¼‰å…¥å„ªåŒ–æ¨¡å‹: {latest_model_file}")
            return model_package['model']

        except Exception as e:
            print(f"è¼‰å…¥å„ªåŒ–æ¨¡å‹å¤±æ•—: {e}")
            return None
        
    def _init_quantitative_indicators(self):
        """åˆå§‹åŒ–7ç¶­åº¦é‡åŒ–æŒ‡æ¨™ç³»çµ±"""
        return {
            'population_structure': {
                'eligible_voters': 0,
                'age_distribution': {},
                'geographic_distribution': {},
                'mobility_factor': 0  # äººå£æµå‹•ä¿‚æ•¸
            },
            'election_history': {
                'previous_turnout': 0,
                'previous_support_rate': 0,
                'historical_pattern': {}
            },
            'online_sentiment': {
                'discussion_volume': 0,
                'sentiment_ratio': 0,
                'trend_momentum': 0
            },
            'social_mobilization': {
                'post_engagement': 0,
                'community_activity': 0,
                'influencer_support': 0
            },
            'weather_factor': {
                'weather_score': 0,
                'precipitation_prob': 0,
                'temperature_comfort': 0
            },
            'event_impact': {
                'negative_keyword_spike': 0,
                'controversy_level': 0,
                'media_attention': 0
            },
            'political_mobilization': {
                'party_support': 0,
                'campaign_intensity': 0,
                'endorsement_count': 0
            }
        }

    def _load_demographic_keywords(self):
        """è¼‰å…¥äººå£çµ±è¨ˆé—œéµè© - æ“´å±•ç‰ˆæœ¬"""
        return {
            'age_groups': {
                'young': ['å­¸ç”Ÿ', 'å¤§å­¸', 'å¹´è¼•', 'æ–°é®®äºº', 'å‰›ç•¢æ¥­', '20æ­²', 'å¤§ä¸€', 'å¤§äºŒ', 'å¤§ä¸‰', 'å¤§å››',
                         'é’å¹´', 'å¹´è¼•äºº', 'æ–°ä¸–ä»£', '90å¾Œ', '00å¾Œ'],
                'middle': ['ä¸Šç­æ—', 'å·¥ä½œ', 'è·å ´', '30æ­²', '40æ­²', 'ä¸­å¹´', 'å®¶åº­', 'å°å­©', 'çˆ¶æ¯',
                          'ä¸­å£¯å¹´', 'ç¤¾æœƒäººå£«', 'è·æ¥­å©¦å¥³'],
                'senior': ['é€€ä¼‘', 'è€äºº', 'é•·è¼©', '50æ­²', '60æ­²', '70æ­²', 'é˜¿å…¬', 'é˜¿å¬¤', 'éŠ€é«®æ—',
                          'è³‡æ·±', 'å¹´é•·è€…', 'é•·è€…']
            },
            'regions': {
                'north': ['å°åŒ—', 'æ–°åŒ—', 'æ¡ƒåœ’', 'æ–°ç«¹', 'åŸºéš†', 'å®œè˜­', 'åŒ—éƒ¨', 'å¤§å°åŒ—'],
                'central': ['å°ä¸­', 'å½°åŒ–', 'å—æŠ•', 'é›²æ—', 'è‹—æ —', 'ä¸­éƒ¨', 'ä¸­å°ç£'],
                'south': ['å°å—', 'é«˜é›„', 'å±æ±', 'å˜‰ç¾©', 'å—éƒ¨', 'å—å°ç£'],
                'east': ['èŠ±è“®', 'å°æ±', 'æ±éƒ¨', 'æ±å°ç£'],
                'islands': ['æ¾æ¹–', 'é‡‘é–€', 'é¦¬ç¥–', 'é›¢å³¶']
            },
            'occupation': {
                'student': ['å­¸ç”Ÿ', 'å¤§å­¸ç”Ÿ', 'ç ”ç©¶ç”Ÿ', 'åšå£«ç”Ÿ', 'åœ¨å­¸', 'æ±‚å­¸'],
                'professional': ['å·¥ç¨‹å¸«', 'é†«å¸«', 'å¾‹å¸«', 'æœƒè¨ˆå¸«', 'è€å¸«', 'æ•™æˆ', 'å°ˆæ¥­äººå£«'],
                'business': ['è€é—†', 'å‰µæ¥­', 'è‡ªç‡Ÿ', 'å•†äºº', 'æ¥­å‹™', 'ä¼æ¥­ä¸»'],
                'labor': ['å·¥äºº', 'ä½œæ¥­å“¡', 'å¸æ©Ÿ', 'æœå‹™æ¥­', 'é¤é£²', 'å‹å·¥'],
                'government': ['å…¬å‹™å“¡', 'è»äºº', 'è­¦å¯Ÿ', 'æ¶ˆé˜²å“¡', 'å…¬è·', 'è»å…¬æ•™']
            },
            'voting_motivation': {
                'high': ['ä¸€å®šè¦æŠ•', 'å¿…é ˆæŠ•ç¥¨', 'è²¬ä»»', 'ç¾©å‹™', 'é—œå¿ƒæ”¿æ²»', 'å…¬æ°‘è²¬ä»»'],
                'medium': ['å¯èƒ½æœƒæŠ•', 'çœ‹æƒ…æ³', 'æœ‰ç©ºå°±å»', 'è€ƒæ…®ä¸­'],
                'low': ['ä¸æƒ³æŠ•', 'æ‡¶å¾—æŠ•', 'æ²’èˆˆè¶£', 'ç„¡æ‰€è¬‚', 'ä¸é—œå¿ƒ']
            }
        }
    
    def _load_issue_keywords(self):
        """è¼‰å…¥è­°é¡Œé—œéµè© - æ“´å±•ç‰ˆæœ¬"""
        return {
            'political_issues': {
                'governance': ['æ–½æ”¿', 'æ”¿ç­–', 'æ²»ç†', 'è¡Œæ”¿', 'æ•ˆç‡', 'èƒ½åŠ›', 'åŸ·æ”¿', 'ç®¡ç†'],
                'corruption': ['è²ªæ±¡', 'è…æ•—', 'å¼Šæ¡ˆ', 'é»‘é‡‘', 'åˆ©ç›Š', 'é—œèªª', 'è²ªè…', 'æ”¶è³„'],
                'democracy': ['æ°‘ä¸»', 'è‡ªç”±', 'äººæ¬Š', 'æ³•æ²»', 'é€æ˜', 'ç›£ç£', 'æ°‘ä¸»åˆ¶åº¦'],
                'economy': ['ç¶“æ¿Ÿ', 'å°±æ¥­', 'è–ªè³‡', 'ç‰©åƒ¹', 'æˆ¿åƒ¹', 'æŠ•è³‡', 'æ™¯æ°£', 'å¤±æ¥­']
            },
            'recall_reasons': {
                'performance': ['ç„¡èƒ½', 'å¤±è·', 'ä¸é©ä»»', 'è¡¨ç¾å·®', 'æ²’åšäº‹', 'ä¸ç¨±è·', 'èƒ½åŠ›ä¸è¶³'],
                'scandal': ['é†œè', 'çˆ†æ–™', 'è² é¢', 'çˆ­è­°', 'å•é¡Œ', 'é¢¨æ³¢', 'äº‹ä»¶'],
                'ideology': ['ç†å¿µ', 'åƒ¹å€¼è§€', 'ç«‹å ´', 'æ”¿æ²»', 'æ„è­˜å½¢æ…‹', 'æ”¿æ²»ç«‹å ´'],
                'representation': ['ä»£è¡¨æ€§', 'æ°‘æ„', 'é¸æ°‘', 'æ‰¿è«¾', 'èƒŒå›', 'æ°‘æ„ä»£è¡¨', 'é¸æ°‘æ„å¿—']
            },
            'mobilization_keywords': {
                'support_mobilization': ['æ”¯æŒ', 'æŒº', 'ç«™å‡ºä¾†', 'åœ˜çµ', 'ä¸€èµ·', 'åŠ æ²¹', 'åŠªåŠ›'],
                'oppose_mobilization': ['åå°', 'æŠµåˆ¶', 'æ‹’çµ•', 'ä¸è¦', 'é˜»æ­¢', 'åç½·å…'],
                'neutral_discussion': ['è¨è«–', 'åˆ†æ', 'æ€è€ƒ', 'è§€å¯Ÿ', 'äº†è§£', 'é—œæ³¨']
            },
            'urgency_keywords': {
                'high_urgency': ['ç·Šæ€¥', 'é‡è¦', 'é—œéµ', 'æ±ºå®šæ€§', 'æœ€å¾Œæ©Ÿæœƒ', 'ä¸èƒ½éŒ¯é'],
                'medium_urgency': ['éœ€è¦', 'æ‡‰è©²', 'å»ºè­°', 'å¸Œæœ›', 'æœŸå¾…'],
                'low_urgency': ['å¯ä»¥', 'æˆ–è¨±', 'ä¹Ÿè¨±', 'è€ƒæ…®', 'çœ‹çœ‹']
            }
        }
    
    def classify_demographics(self, df, text_column='content'):
        """äººå£çµ±è¨ˆåˆ†é¡ (MECE: å¹´é½¡ã€åœ°å€ã€è·æ¥­)"""
        results = []
        
        for idx, row in df.iterrows():
            text = str(row[text_column]).lower()
            
            # å¹´é½¡åˆ†é¡
            age_group = 'unknown'
            for group, keywords in self.demographic_keywords['age_groups'].items():
                if any(keyword in text for keyword in keywords):
                    age_group = group
                    break
            
            # åœ°å€åˆ†é¡
            region = 'unknown'
            for reg, cities in self.demographic_keywords['regions'].items():
                if any(city in text for city in cities):
                    region = reg
                    break
            
            # è·æ¥­åˆ†é¡
            occupation = 'unknown'
            for occ, keywords in self.demographic_keywords['occupation'].items():
                if any(keyword in text for keyword in keywords):
                    occupation = occ
                    break
            
            results.append({
                'index': idx,
                'age_group': age_group,
                'region': region,
                'occupation': occupation
            })
        
        return pd.DataFrame(results)

    def predict_turnout_rate(self, df, sentiment_df, demo_df, weather_data=None):
        """é æ¸¬æŠ•ç¥¨ç‡ - åŸºæ–¼MECEæ¡†æ¶çš„ä¸‰å¤§å› ç´ """

        # A. çµæ§‹æ€§å› ç´ åˆ†æ
        structural_factors = self._analyze_structural_factors(df, demo_df)

        # B. å‹•æ©Ÿå› ç´ åˆ†æ
        motivation_factors = self._analyze_motivation_factors(df, sentiment_df)

        # C. ç¤¾ç¾¤åª’é«”è²é‡åˆ†æ
        social_media_factors = self._analyze_social_media_factors(df, sentiment_df)

        # D. å¤©æ°£å› ç´  (å¦‚æœæœ‰è³‡æ–™)
        weather_impact = self._analyze_weather_impact(weather_data) if weather_data else 0.0

        # ç¶œåˆè¨ˆç®—æŠ•ç¥¨ç‡é æ¸¬
        base_turnout = 0.45  # åŸºç¤æŠ•ç¥¨ç‡å‡è¨­

        # å„å› ç´ æ¬Šé‡
        structural_weight = 0.3
        motivation_weight = 0.4
        social_weight = 0.2
        weather_weight = 0.1

        predicted_turnout = (
            base_turnout +
            structural_factors * structural_weight +
            motivation_factors * motivation_weight +
            social_media_factors * social_weight +
            weather_impact * weather_weight
        )

        # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
        predicted_turnout = max(0.1, min(0.9, predicted_turnout))

        return {
            'predicted_turnout_rate': predicted_turnout,
            'structural_score': structural_factors,
            'motivation_score': motivation_factors,
            'social_media_score': social_media_factors,
            'weather_impact': weather_impact,
            'confidence_level': self._calculate_turnout_confidence(df)
        }

    def _analyze_structural_factors(self, df, demo_df):
        """åˆ†æçµæ§‹æ€§å› ç´  (äººå£èˆ‡åœ°ç†)"""
        factors = []

        # åœ°ç†ä¾¿åˆ©æ€§ (æŠ•é–‹ç¥¨æ‰€å¯†åº¦ä»£ç†æŒ‡æ¨™)
        region_distribution = demo_df['region'].value_counts(normalize=True)
        urban_ratio = region_distribution.get('north', 0) + region_distribution.get('central', 0)
        factors.append(urban_ratio * 0.1)  # éƒ½å¸‚åŒ–ç¨‹åº¦æå‡æŠ•ç¥¨ç‡

        # äººå£æµå‹•å½±éŸ¿
        if 'source' in df.columns:
            local_discussion_ratio = len(df[df['source'] == 'PTT']) / len(df) if len(df) > 0 else 0
            factors.append(local_discussion_ratio * 0.05)

        return sum(factors)

    def _analyze_motivation_factors(self, df, sentiment_df):
        """åˆ†æå‹•æ©Ÿå› ç´ """
        factors = []

        # æ”¿æ²»å‹•å“¡ç¨‹åº¦
        if 'recall_stance' in sentiment_df.columns:
            stance_distribution = sentiment_df['recall_stance'].value_counts(normalize=True)
            polarization = 1 - max(stance_distribution) if len(stance_distribution) > 1 else 0
            factors.append(polarization * 0.15)  # æ¥µåŒ–ç¨‹åº¦æå‡æŠ•ç¥¨ç‡

        # è­°é¡Œç†±åº¦
        if 'sentiment_score' in sentiment_df.columns:
            avg_sentiment_intensity = abs(sentiment_df['sentiment_score'].mean())
            factors.append(avg_sentiment_intensity * 0.1)

        # è¨è«–æ´»èºåº¦
        discussion_volume = len(df) / 1000  # æ¨™æº–åŒ–è¨è«–é‡
        factors.append(min(discussion_volume, 0.1))

        return sum(factors)

    def _analyze_social_media_factors(self, df, sentiment_df):
        """åˆ†æç¤¾ç¾¤åª’é«”è²é‡"""
        factors = []

        # PTT vs Dcard æ´»èºåº¦å·®ç•°
        if 'source' in df.columns:
            source_counts = df['source'].value_counts()
            total_posts = len(df)
            if total_posts > 0:
                ptt_ratio = source_counts.get('PTT', 0) / total_posts
                dcard_ratio = source_counts.get('Dcard', 0) / total_posts
                platform_diversity = 1 - abs(ptt_ratio - dcard_ratio)
                factors.append(platform_diversity * 0.05)

        # æƒ…ç·’å¼·åº¦è®ŠåŒ–
        if 'sentiment_score' in sentiment_df.columns and len(sentiment_df) > 1:
            sentiment_std = sentiment_df['sentiment_score'].std()
            factors.append(min(sentiment_std, 0.1))

        return sum(factors)

    def _analyze_weather_impact(self, weather_data):
        """åˆ†æå¤©æ°£å½±éŸ¿ (è² é¢å½±éŸ¿)"""
        if not weather_data:
            return 0.0

        impact = 0.0

        # é™é›¨æ©Ÿç‡å½±éŸ¿
        if 'precipitation_prob' in weather_data:
            rain_impact = -weather_data['precipitation_prob'] * 0.001  # æ¯1%é™é›¨æ©Ÿç‡æ¸›å°‘0.1%æŠ•ç¥¨ç‡
            impact += rain_impact

        # æ¥µç«¯æº«åº¦å½±éŸ¿
        if 'temperature' in weather_data:
            temp = weather_data['temperature']
            if temp < 10 or temp > 35:  # æ¥µç«¯æº«åº¦
                impact -= 0.02

        return impact

    def _calculate_turnout_confidence(self, df):
        """è¨ˆç®—æŠ•ç¥¨ç‡é æ¸¬ä¿¡å¿ƒåº¦"""
        sample_size = len(df)

        if sample_size < 100:
            return 0.3
        elif sample_size < 500:
            return 0.6
        elif sample_size < 1000:
            return 0.8
        else:
            return 0.9

    def classify_issues(self, df, text_column='content'):
        """è­°é¡Œåˆ†é¡ (MECE: æ”¿æ²»è­°é¡Œã€ç½·å…åŸå› )"""
        results = []
        
        for idx, row in df.iterrows():
            text = str(row[text_column]).lower()
            
            # æ”¿æ²»è­°é¡Œåˆ†é¡
            political_issues = []
            for issue, keywords in self.issue_keywords['political_issues'].items():
                if any(keyword in text for keyword in keywords):
                    political_issues.append(issue)
            
            # ç½·å…åŸå› åˆ†é¡
            recall_reasons = []
            for reason, keywords in self.issue_keywords['recall_reasons'].items():
                if any(keyword in text for keyword in keywords):
                    recall_reasons.append(reason)
            
            results.append({
                'index': idx,
                'political_issues': ','.join(political_issues) if political_issues else 'none',
                'recall_reasons': ','.join(recall_reasons) if recall_reasons else 'none',
                'issue_count': len(political_issues) + len(recall_reasons)
            })
        
        return pd.DataFrame(results)
    
    def temporal_analysis(self, df, date_column='date'):
        """æ™‚é–“åºåˆ—åˆ†æ"""
        if date_column not in df.columns:
            print(f"æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ {date_column}")
            return pd.DataFrame()
        
        # è½‰æ›æ—¥æœŸæ ¼å¼
        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
        df = df.dropna(subset=[date_column])
        
        # æŒ‰æ—¥æœŸåˆ†çµ„
        daily_stats = df.groupby(df[date_column].dt.date).agg({
            'sentiment_score': ['mean', 'std', 'count'],
            'recall_stance': lambda x: (x == 'support_recall').sum() / len(x) if len(x) > 0 else 0
        }).round(3)
        
        daily_stats.columns = ['avg_sentiment', 'sentiment_std', 'post_count', 'support_ratio']
        
        return daily_stats.reset_index()

    def predict_recall_outcome(self, df, sentiment_df, demo_df, issue_df, turnout_prediction):
        """é æ¸¬ç½·å…çµæœ - åŸºæ–¼MECEæ¡†æ¶çš„ä¸‰å¤§ç¶­åº¦"""

        # A. æ³•è¦èˆ‡é–€æª»åˆ†æ
        threshold_analysis = self._analyze_legal_threshold(turnout_prediction)

        # B. é¸æ°‘çµæ§‹åˆ†æ
        voter_structure = self._analyze_voter_structure(demo_df, sentiment_df)

        # C. æƒ…ç·’èˆ‡äº‹ä»¶è®Šæ•¸åˆ†æ
        emotional_events = self._analyze_emotional_events(df, sentiment_df, issue_df)

        # ç¶œåˆé æ¸¬è¨ˆç®—
        base_support_rate = 0.4  # åŸºç¤æ”¯æŒç‡å‡è¨­

        # å„å› ç´ æ¬Šé‡
        threshold_weight = 0.3
        structure_weight = 0.4
        emotion_weight = 0.3

        predicted_support = (
            base_support_rate +
            threshold_analysis['threshold_effect'] * threshold_weight +
            voter_structure['structure_effect'] * structure_weight +
            emotional_events['emotion_effect'] * emotion_weight
        )

        # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
        predicted_support = max(0.1, min(0.9, predicted_support))

        # è¨ˆç®—æ˜¯å¦é€šéç½·å…
        turnout_rate = turnout_prediction['predicted_turnout_rate']
        support_votes_ratio = predicted_support * turnout_rate
        threshold_required = self.recall_threshold  # 25%

        will_pass = support_votes_ratio >= threshold_required

        return {
            'predicted_support_rate': predicted_support,
            'support_votes_ratio': support_votes_ratio,
            'legal_threshold': threshold_required,
            'prediction_result': 'é€šé' if will_pass else 'å¤±æ•—',
            'pass_probability': min(support_votes_ratio / threshold_required, 1.0),
            'safety_margin': support_votes_ratio - threshold_required,
            'confidence_score': self._calculate_outcome_confidence(df, sentiment_df)
        }

    def _analyze_legal_threshold(self, turnout_prediction):
        """åˆ†ææ³•è¦é–€æª»å½±éŸ¿"""
        turnout_rate = turnout_prediction['predicted_turnout_rate']

        # æŠ•ç¥¨ç‡å°é–€æª»é”æˆçš„å½±éŸ¿
        if turnout_rate < 0.3:
            threshold_effect = -0.1  # ä½æŠ•ç¥¨ç‡ä¸åˆ©ç½·å…
        elif turnout_rate > 0.6:
            threshold_effect = 0.05   # é«˜æŠ•ç¥¨ç‡æœ‰åˆ©ç½·å…
        else:
            threshold_effect = 0.0

        return {
            'threshold_effect': threshold_effect,
            'turnout_impact': turnout_rate,
            'threshold_difficulty': self.recall_threshold / turnout_rate if turnout_rate > 0 else float('inf')
        }

    def _analyze_voter_structure(self, demo_df, sentiment_df):
        """åˆ†æé¸æ°‘çµæ§‹"""
        structure_effects = []

        # å¹´é½¡çµæ§‹å½±éŸ¿
        if 'age_group' in demo_df.columns:
            age_dist = demo_df['age_group'].value_counts(normalize=True)
            young_ratio = age_dist.get('young', 0)
            senior_ratio = age_dist.get('senior', 0)

            # å¹´è¼•äººé€šå¸¸æ›´æ”¯æŒç½·å…
            age_effect = (young_ratio - senior_ratio) * 0.1
            structure_effects.append(age_effect)

        # åœ°å€çµæ§‹å½±éŸ¿
        if 'region' in demo_df.columns:
            region_dist = demo_df['region'].value_counts(normalize=True)
            urban_ratio = region_dist.get('north', 0) + region_dist.get('central', 0)

            # éƒ½å¸‚åœ°å€é€šå¸¸æ”¿æ²»åƒèˆ‡åº¦è¼ƒé«˜
            region_effect = urban_ratio * 0.05
            structure_effects.append(region_effect)

        # è·æ¥­çµæ§‹å½±éŸ¿
        if 'occupation' in demo_df.columns:
            occ_dist = demo_df['occupation'].value_counts(normalize=True)
            professional_ratio = occ_dist.get('professional', 0)
            government_ratio = occ_dist.get('government', 0)

            # å°ˆæ¥­äººå£«vså…¬å‹™å“¡çš„å°æ¯”
            occupation_effect = (professional_ratio - government_ratio) * 0.08
            structure_effects.append(occupation_effect)

        return {
            'structure_effect': sum(structure_effects),
            'age_factor': structure_effects[0] if len(structure_effects) > 0 else 0,
            'region_factor': structure_effects[1] if len(structure_effects) > 1 else 0,
            'occupation_factor': structure_effects[2] if len(structure_effects) > 2 else 0
        }

    def _analyze_emotional_events(self, df, sentiment_df, issue_df):
        """åˆ†ææƒ…ç·’èˆ‡äº‹ä»¶è®Šæ•¸"""
        emotion_effects = []

        # è² é¢äº‹ä»¶è¡æ“Š
        if 'recall_reasons' in issue_df.columns:
            scandal_mentions = issue_df['recall_reasons'].str.contains('scandal', na=False).sum()
            scandal_ratio = scandal_mentions / len(issue_df) if len(issue_df) > 0 else 0
            scandal_effect = scandal_ratio * 0.15
            emotion_effects.append(scandal_effect)

        # æƒ…ç·’æ¥µåŒ–ç¨‹åº¦
        if 'sentiment_score' in sentiment_df.columns:
            sentiment_std = sentiment_df['sentiment_score'].std()
            polarization_effect = min(sentiment_std, 0.5) * 0.1
            emotion_effects.append(polarization_effect)

        # è¨è«–ç†±åº¦çªå¢
        discussion_intensity = len(df) / 1000  # æ¨™æº–åŒ–
        intensity_effect = min(discussion_intensity, 0.1)
        emotion_effects.append(intensity_effect)

        return {
            'emotion_effect': sum(emotion_effects),
            'scandal_impact': emotion_effects[0] if len(emotion_effects) > 0 else 0,
            'polarization_impact': emotion_effects[1] if len(emotion_effects) > 1 else 0,
            'intensity_impact': emotion_effects[2] if len(emotion_effects) > 2 else 0
        }

    def _calculate_outcome_confidence(self, df, sentiment_df):
        """è¨ˆç®—çµæœé æ¸¬ä¿¡å¿ƒåº¦"""
        factors = []

        # æ¨£æœ¬å¤§å°
        sample_size = len(df)
        if sample_size > 1000:
            factors.append(0.3)
        elif sample_size > 500:
            factors.append(0.2)
        else:
            factors.append(0.1)

        # ç«‹å ´åˆ†å¸ƒå‡è¡¡åº¦
        if 'recall_stance' in sentiment_df.columns:
            stance_counts = sentiment_df['recall_stance'].value_counts()
            if len(stance_counts) > 1:
                balance = 1 - abs(stance_counts.iloc[0] - stance_counts.iloc[1]) / len(sentiment_df)
                factors.append(balance * 0.3)

        # æ™‚é–“è·¨åº¦ (å¦‚æœæœ‰æ—¥æœŸè³‡æ–™)
        if 'date' in df.columns:
            factors.append(0.2)  # æœ‰æ™‚é–“åºåˆ—è³‡æ–™å¢åŠ ä¿¡å¿ƒåº¦

        return min(sum(factors), 0.9)

    def create_prediction_features(self, df, sentiment_df, demo_df, issue_df):
        """å‰µå»ºé æ¸¬ç‰¹å¾µ"""
        # åˆä½µæ‰€æœ‰åˆ†æçµæœ
        combined_df = df.copy()
        
        # åŠ å…¥æƒ…ç·’åˆ†æçµæœ
        for col in ['sentiment', 'sentiment_score', 'recall_stance', 'stance_confidence']:
            if col in sentiment_df.columns:
                combined_df[col] = sentiment_df[col]
        
        # åŠ å…¥äººå£çµ±è¨ˆåˆ†é¡
        for col in ['age_group', 'region', 'occupation']:
            if col in demo_df.columns:
                combined_df[col] = demo_df[col]
        
        # åŠ å…¥è­°é¡Œåˆ†é¡
        for col in ['political_issues', 'recall_reasons', 'issue_count']:
            if col in issue_df.columns:
                combined_df[col] = issue_df[col]
        
        # å‰µå»ºç‰¹å¾µå·¥ç¨‹
        features = []
        
        for idx, row in combined_df.iterrows():
            feature_dict = {
                # åŸºç¤ç‰¹å¾µ
                'sentiment_score': row.get('sentiment_score', 0),
                'stance_confidence': row.get('stance_confidence', 0),
                'issue_count': row.get('issue_count', 0),
                
                # ä¾†æºç‰¹å¾µ
                'source_ptt': 1 if row.get('source') == 'PTT' else 0,
                'source_dcard': 1 if row.get('source') == 'Dcard' else 0,
                
                # äººå£çµ±è¨ˆç‰¹å¾µ
                'age_young': 1 if row.get('age_group') == 'young' else 0,
                'age_middle': 1 if row.get('age_group') == 'middle' else 0,
                'age_senior': 1 if row.get('age_group') == 'senior' else 0,
                
                'region_north': 1 if row.get('region') == 'north' else 0,
                'region_central': 1 if row.get('region') == 'central' else 0,
                'region_south': 1 if row.get('region') == 'south' else 0,
                
                # è­°é¡Œç‰¹å¾µ
                'issue_governance': 1 if 'governance' in str(row.get('political_issues', '')) else 0,
                'issue_corruption': 1 if 'corruption' in str(row.get('political_issues', '')) else 0,
                'issue_democracy': 1 if 'democracy' in str(row.get('political_issues', '')) else 0,
                
                # ç›®æ¨™è®Šæ•¸
                'support_recall': 1 if row.get('recall_stance') == 'support_recall' else 0
            }
            
            features.append(feature_dict)
        
        return pd.DataFrame(features)
    
    def build_prediction_model(self, features_df):
        """å»ºç«‹é æ¸¬æ¨¡å‹"""
        # æº–å‚™ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
        feature_columns = [col for col in features_df.columns if col != 'support_recall']
        X = features_df[feature_columns]
        y = features_df['support_recall']
        
        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦é›†
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # å˜—è©¦è¼‰å…¥å„ªåŒ–å¾Œçš„æ¨¡å‹
        optimized_model = self._load_optimized_model()

        if optimized_model:
            model = optimized_model
            self.logger.info("ä½¿ç”¨å„ªåŒ–å¾Œçš„æ¨¡å‹é€²è¡Œé æ¸¬")
        else:
            # è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)
            self.logger.info("ä½¿ç”¨åŸºç¤æ¨¡å‹é€²è¡Œé æ¸¬")
        
        # é æ¸¬å’Œè©•ä¼°
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        # ç‰¹å¾µé‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': feature_columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return {
            'model': model,
            'accuracy': accuracy,
            'feature_importance': feature_importance,
            'classification_report': classification_report(y_test, y_pred)
        }
    


    def predict_recall_outcome_ml(self, features_df, model_results):
        """æ©Ÿå™¨å­¸ç¿’é æ¸¬ç½·å…çµæœ (é‡æ–°å‘½åä»¥é¿å…è¡çª)"""
        model = model_results['model']

        # è¨ˆç®—æ”¯æŒç‡
        feature_columns = [col for col in features_df.columns if col != 'support_recall']
        X = features_df[feature_columns]

        # é æ¸¬æ¦‚ç‡ - è™•ç†å–®é¡åˆ¥æƒ…æ³
        proba = model.predict_proba(X)
        if proba.shape[1] == 1:
            # åªæœ‰ä¸€å€‹é¡åˆ¥ï¼Œä½¿ç”¨é æ¸¬å€¼ä½œç‚ºæ”¯æŒç‡
            predictions = model.predict(X)
            support_probs = predictions.astype(float)
        else:
            # æœ‰å…©å€‹é¡åˆ¥ï¼Œå–æ”¯æŒé¡åˆ¥çš„æ¦‚ç‡
            support_probs = proba[:, 1]

        # è¨ˆç®—æ•´é«”æ”¯æŒç‡
        overall_support_rate = float(support_probs.mean())

        # é æ¸¬çµæœ
        prediction = {
            'support_rate': overall_support_rate,
            'prediction': 'PASS' if overall_support_rate > 0.5 else 'FAIL',
            'confidence': float(abs(overall_support_rate - 0.5) * 2),
            'sample_size': int(len(features_df))
        }

        return prediction

    def create_mece_visualizations(self, df, sentiment_df, demo_df, issue_df):
        """å‰µå»ºMECEåˆ†æè¦–è¦ºåŒ–"""
        fig, axes = plt.subplots(3, 2, figsize=(16, 18))
        
        # 1. å¹´é½¡ç¾¤çµ„vsæƒ…ç·’
        age_sentiment = pd.merge(demo_df, sentiment_df, on='index')
        age_groups = age_sentiment.groupby('age_group')['sentiment_score'].mean()
        axes[0, 0].bar(age_groups.index, age_groups.values)
        axes[0, 0].set_title('å„å¹´é½¡ç¾¤çµ„å¹³å‡æƒ…ç·’åˆ†æ•¸')
        axes[0, 0].set_ylabel('æƒ…ç·’åˆ†æ•¸')
        
        # 2. åœ°å€vsç½·å…ç«‹å ´
        region_stance = pd.merge(demo_df, sentiment_df, on='index')
        stance_by_region = region_stance.groupby(['region', 'recall_stance']).size().unstack(fill_value=0)
        stance_by_region.plot(kind='bar', ax=axes[0, 1], stacked=True)
        axes[0, 1].set_title('å„åœ°å€ç½·å…ç«‹å ´åˆ†å¸ƒ')
        axes[0, 1].legend(title='ç«‹å ´')
        
        # 3. è·æ¥­vsæƒ…ç·’
        occ_sentiment = pd.merge(demo_df, sentiment_df, on='index')
        occ_groups = occ_sentiment.groupby('occupation')['sentiment_score'].mean()
        axes[1, 0].bar(occ_groups.index, occ_groups.values)
        axes[1, 0].set_title('å„è·æ¥­ç¾¤çµ„å¹³å‡æƒ…ç·’åˆ†æ•¸')
        axes[1, 0].set_ylabel('æƒ…ç·’åˆ†æ•¸')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. è­°é¡Œé—œæ³¨åº¦
        issue_counts = issue_df['political_issues'].str.split(',').explode().value_counts()
        issue_counts = issue_counts[issue_counts.index != 'none'][:10]
        axes[1, 1].barh(issue_counts.index, issue_counts.values)
        axes[1, 1].set_title('æ”¿æ²»è­°é¡Œé—œæ³¨åº¦æ’å')
        axes[1, 1].set_xlabel('æåŠæ¬¡æ•¸')
        
        # 5. æ™‚é–“è¶¨å‹¢ (å¦‚æœæœ‰æ—¥æœŸè³‡æ–™)
        if 'date' in df.columns:
            temporal_data = self.temporal_analysis(pd.merge(df, sentiment_df, left_index=True, right_on='index'))
            if not temporal_data.empty:
                axes[2, 0].plot(temporal_data['date'], temporal_data['avg_sentiment'])
                axes[2, 0].set_title('æƒ…ç·’è¶¨å‹¢è®ŠåŒ–')
                axes[2, 0].set_ylabel('å¹³å‡æƒ…ç·’åˆ†æ•¸')
                axes[2, 0].tick_params(axis='x', rotation=45)
        
        # 6. ä¾†æºvsç«‹å ´
        if 'source' in df.columns:
            source_stance = pd.merge(df, sentiment_df, left_index=True, right_on='index')
            source_stance_dist = source_stance.groupby(['source', 'recall_stance']).size().unstack(fill_value=0)
            source_stance_dist.plot(kind='bar', ax=axes[2, 1])
            axes[2, 1].set_title('å„è³‡æ–™ä¾†æºç½·å…ç«‹å ´åˆ†å¸ƒ')
            axes[2, 1].legend(title='ç«‹å ´')
        
        plt.tight_layout()
        plt.savefig('mece_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    analyzer = MECEAnalyzer()
    
    # è¼‰å…¥æœ€æ–°çš„æƒ…ç·’åˆ†æçµæœ
    import glob
    import os
    
    result_files = glob.glob("sentiment_analysis_results_*.csv")
    if not result_files:
        print("æ‰¾ä¸åˆ°æƒ…ç·’åˆ†æçµæœæª”æ¡ˆï¼Œè«‹å…ˆåŸ·è¡Œ sentiment_analyzer.py")
        return
    
    latest_file = max(result_files, key=os.path.getctime)
    print(f"è¼‰å…¥åˆ†æçµæœ: {latest_file}")
    
    df = pd.read_csv(latest_file)
    print(f"è¼‰å…¥ {len(df)} ç­†è³‡æ–™")
    
    # åˆ†é›¢åŸå§‹è³‡æ–™å’Œæƒ…ç·’åˆ†æçµæœ
    sentiment_columns = ['sentiment', 'sentiment_score', 'sentiment_confidence', 
                        'recall_stance', 'stance_confidence']
    sentiment_df = df[['index'] + [col for col in sentiment_columns if col in df.columns]].copy()
    original_df = df.drop(columns=[col for col in sentiment_columns if col in df.columns])
    
    # åŸ·è¡ŒMECEåˆ†æ
    print("åŸ·è¡Œäººå£çµ±è¨ˆåˆ†é¡...")
    demo_df = analyzer.classify_demographics(original_df)

    print("åŸ·è¡Œè­°é¡Œåˆ†é¡...")
    issue_df = analyzer.classify_issues(original_df)

    # æ–°å¢ï¼šæŠ•ç¥¨ç‡é æ¸¬
    print("é æ¸¬æŠ•ç¥¨ç‡...")
    turnout_prediction = analyzer.predict_turnout_rate(original_df, sentiment_df, demo_df)

    # æ–°å¢ï¼šç½·å…çµæœé æ¸¬
    print("é æ¸¬ç½·å…çµæœ...")
    outcome_prediction = analyzer.predict_recall_outcome(original_df, sentiment_df, demo_df, issue_df, turnout_prediction)

    print("å‰µå»ºé æ¸¬ç‰¹å¾µ...")
    features_df = analyzer.create_prediction_features(original_df, sentiment_df, demo_df, issue_df)

    print("å»ºç«‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹...")
    model_results = analyzer.build_prediction_model(features_df)

    print("æ©Ÿå™¨å­¸ç¿’é æ¸¬...")
    ml_prediction = analyzer.predict_recall_outcome_ml(features_df, model_results)
    
    # è¼¸å‡ºçµæœ
    print("\n" + "="*60)
    print("ğŸ¯ å°ç£ç½·å…é æ¸¬åˆ†æçµæœ (åŸºæ–¼å®Œæ•´MECEæ¡†æ¶)")
    print("="*60)

    print("\nğŸ“Š æŠ•ç¥¨ç‡é æ¸¬:")
    print(f"  é æ¸¬æŠ•ç¥¨ç‡: {turnout_prediction['predicted_turnout_rate']:.1%}")
    print(f"  çµæ§‹æ€§å› ç´ : {turnout_prediction['structural_score']:.3f}")
    print(f"  å‹•æ©Ÿå› ç´ : {turnout_prediction['motivation_score']:.3f}")
    print(f"  ç¤¾ç¾¤åª’é«”å› ç´ : {turnout_prediction['social_media_score']:.3f}")
    print(f"  ä¿¡å¿ƒåº¦: {turnout_prediction['confidence_level']:.1%}")

    print("\nğŸ—³ï¸ ç½·å…çµæœé æ¸¬:")
    print(f"  é æ¸¬æ”¯æŒç‡: {outcome_prediction['predicted_support_rate']:.1%}")
    print(f"  æ”¯æŒç¥¨æ¯”ä¾‹: {outcome_prediction['support_votes_ratio']:.1%}")
    print(f"  æ³•å®šé–€æª»: {outcome_prediction['legal_threshold']:.1%}")
    print(f"  é æ¸¬çµæœ: {'âœ… ç½·å…é€šé' if outcome_prediction['prediction_result'] == 'é€šé' else 'âŒ ç½·å…å¤±æ•—'}")
    print(f"  é€šéæ©Ÿç‡: {outcome_prediction['pass_probability']:.1%}")
    print(f"  å®‰å…¨é‚Šéš›: {outcome_prediction['safety_margin']:+.1%}")
    print(f"  ä¿¡å¿ƒåº¦: {outcome_prediction['confidence_score']:.1%}")

    print("\nğŸ¤– æ©Ÿå™¨å­¸ç¿’æ¨¡å‹:")
    print(f"  æ¨¡å‹æº–ç¢ºç‡: {model_results['accuracy']:.1%}")
    print(f"  MLé æ¸¬æ”¯æŒç‡: {ml_prediction['support_rate']:.1%}")
    print(f"  MLé æ¸¬çµæœ: {ml_prediction['prediction']}")
    print(f"  MLä¿¡å¿ƒåº¦: {ml_prediction['confidence']:.1%}")

    print("\nğŸ” ç‰¹å¾µé‡è¦æ€§æ’å:")
    print(model_results['feature_importance'].head(10).to_string(index=False))
    
    # å„²å­˜çµæœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å„²å­˜MECEåˆ†æçµæœ
    mece_results = pd.concat([original_df, sentiment_df, demo_df, issue_df], axis=1)
    mece_results.to_csv(f"mece_analysis_results_{timestamp}.csv", index=False, encoding='utf-8-sig')
    
    # å„²å­˜å®Œæ•´é æ¸¬çµæœ
    prediction_summary = {
        'analysis_time': datetime.now().isoformat(),
        'sample_size': int(len(original_df)),

        # æŠ•ç¥¨ç‡é æ¸¬
        'turnout_prediction': {
            'predicted_turnout': float(turnout_prediction['predicted_turnout_rate']),
            'structural_score': float(turnout_prediction['structural_score']),
            'motivation_score': float(turnout_prediction['motivation_score']),
            'social_media_score': float(turnout_prediction['social_media_score']),
            'confidence_level': float(turnout_prediction['confidence_level'])
        },

        # ç½·å…çµæœé æ¸¬ (MECEæ¡†æ¶)
        'outcome_prediction': {
            'predicted_support_rate': float(outcome_prediction['predicted_support_rate']),
            'support_votes_ratio': float(outcome_prediction['support_votes_ratio']),
            'legal_threshold': float(outcome_prediction['legal_threshold']),
            'prediction_result': str(outcome_prediction['prediction_result']),
            'pass_probability': float(outcome_prediction['pass_probability']),
            'safety_margin': float(outcome_prediction['safety_margin']),
            'confidence_score': float(outcome_prediction['confidence_score'])
        },

        # æ©Ÿå™¨å­¸ç¿’é æ¸¬
        'ml_prediction': {
            'model_accuracy': float(model_results['accuracy']),
            'predicted_support_rate': float(ml_prediction['support_rate']),
            'prediction': str(ml_prediction['prediction']),
            'confidence': float(ml_prediction['confidence'])
        },

        # ç‰¹å¾µé‡è¦æ€§
        'feature_importance': [
            {
                'feature': str(row['feature']),
                'importance': float(row['importance'])
            }
            for _, row in model_results['feature_importance'].head(10).iterrows()
        ]
    }
    
    import json
    import numpy as np

    def convert_to_serializable(obj):
        """è½‰æ›ç‚ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        else:
            return obj

    # è½‰æ›ç‚ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_summary = convert_to_serializable(prediction_summary)

    with open(f"prediction_results_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(serializable_summary, f, ensure_ascii=False, indent=2)
    
    # å‰µå»ºè¦–è¦ºåŒ–
    analyzer.create_mece_visualizations(original_df, sentiment_df, demo_df, issue_df)
    
    print(f"\nğŸ’¾ çµæœå·²å„²å­˜:")
    print(f"  ğŸ“Š MECEåˆ†æçµæœ: mece_analysis_results_{timestamp}.csv")
    print(f"  ğŸ¯ é æ¸¬çµæœ: prediction_results_{timestamp}.json")
    print(f"  ğŸ“ˆ è¦–è¦ºåŒ–åœ–è¡¨: mece_analysis.png")

    print(f"\nğŸ‰ åˆ†æå®Œæˆï¼åŸºæ–¼ {len(original_df)} ç­†è³‡æ–™çš„å®Œæ•´MECEæ¡†æ¶é æ¸¬")

    return mece_results, prediction_summary

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬èŸ²ç³»çµ±æ¸¬è©¦
Crawler System Test

æ¸¬è©¦æ•´å€‹æ¨¡çµ„åŒ–çˆ¬èŸ²ç³»çµ±
"""

import sys
import os
import logging
import json
from datetime import datetime

# æ·»åŠ è·¯å¾‘
sys.path.append(os.path.dirname(__file__))

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_config():
    """æ¸¬è©¦é…ç½®æ¨¡çµ„"""
    print("=== æ¸¬è©¦é…ç½®æ¨¡çµ„ ===")
    try:
        from crawler.config import get_config, KEYWORDS
        
        config = get_config()
        print(f"âœ… é…ç½®è¼‰å…¥æˆåŠŸ")
        print(f"é—œéµå­—æ•¸é‡: {len(KEYWORDS['recall'])} + {len(KEYWORDS['candidates'])}")
        print(f"PTTçœ‹æ¿: {config['ptt']['boards']}")
        print(f"Dcardè«–å£‡: {config['dcard']['forums']}")
        
        return True
    except Exception as e:
        print(f"âŒ é…ç½®æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_utils():
    """æ¸¬è©¦å·¥å…·æ¨¡çµ„"""
    print("\n=== æ¸¬è©¦å·¥å…·æ¨¡çµ„ ===")
    try:
        from utils.common import text_processor, date_processor, data_processor
        
        # æ¸¬è©¦æ–‡æœ¬è™•ç†
        test_text = "é€™å€‹å€™é¸äººçœŸçš„å¾ˆçˆ›ï¼Œæˆ‘ä¸æ”¯æŒä»–ï¼"
        sentiment = text_processor.analyze_sentiment(test_text)
        print(f"âœ… æƒ…ç·’åˆ†æ: {sentiment['sentiment']} (åˆ†æ•¸: {sentiment['score']:.2f})")
        
        # æ¸¬è©¦æ—¥æœŸè™•ç†
        test_date = "2å°æ™‚å‰"
        parsed_date = date_processor.parse_date(test_date)
        print(f"âœ… æ—¥æœŸè§£æ: {parsed_date}")
        
        # æ¸¬è©¦æ•¸æ“šè™•ç†
        test_articles = [
            {'title': 'æ¸¬è©¦1', 'content': 'å…§å®¹1'},
            {'title': 'æ¸¬è©¦1', 'content': 'å…§å®¹1'},  # é‡è¤‡
            {'title': 'æ¸¬è©¦2', 'content': 'å…§å®¹2'}
        ]
        unique_articles = data_processor.deduplicate_articles(test_articles)
        print(f"âœ… å»é‡è™•ç†: {len(test_articles)} -> {len(unique_articles)}")
        
        return True
    except Exception as e:
        print(f"âŒ å·¥å…·æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_sqlite_storage():
    """æ¸¬è©¦SQLiteå„²å­˜"""
    print("\n=== æ¸¬è©¦SQLiteå„²å­˜ ===")
    try:
        from storage.sqlite_handler import SQLiteHandler
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“šåº«
        sqlite = SQLiteHandler('test_crawler.db')
        
        # æ¸¬è©¦æ’å…¥
        test_articles = [
            {
                'title': 'æ¸¬è©¦æ–‡ç« ',
                'content': 'é€™æ˜¯æ¸¬è©¦å…§å®¹',
                'author': 'test_user',
                'date': datetime.now().isoformat(),
                'link': 'http://test.com/1',
                'source': 'Test',
                'sentiment': 'positive',
                'sentiment_score': 0.5,
                'keywords_found': ['æ¸¬è©¦']
            }
        ]
        
        result = sqlite.insert_articles(test_articles)
        print(f"âœ… æ’å…¥çµæœ: {result}")
        
        # æ¸¬è©¦æŸ¥è©¢
        articles = sqlite.get_articles(limit=5)
        print(f"âœ… æŸ¥è©¢çµæœ: {len(articles)} ç¯‡æ–‡ç« ")
        
        # æ¸¬è©¦çµ±è¨ˆ
        stats = sqlite.get_statistics()
        print(f"âœ… çµ±è¨ˆçµæœ: {stats['total_articles']} ç¯‡æ–‡ç« ")
        
        sqlite.close()
        return True
    except Exception as e:
        print(f"âŒ SQLiteæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_dcard_crawler():
    """æ¸¬è©¦Dcardçˆ¬èŸ²"""
    print("\n=== æ¸¬è©¦Dcardçˆ¬èŸ² ===")
    try:
        from crawler.dcard_crawler import DcardCrawler
        
        crawler = DcardCrawler()
        
        # æ¸¬è©¦çˆ¬å–ä¸€å€‹è«–å£‡çš„å°‘é‡æ•¸æ“š
        keywords = ['æ”¿æ²»', 'é¸èˆ‰']
        articles = crawler.get_forum_articles('politics', keywords, pages=1)
        
        print(f"âœ… Dcardçˆ¬å–çµæœ: {len(articles)} ç¯‡æ–‡ç« ")
        
        if articles:
            print("å‰3ç¯‡æ–‡ç« :")
            for i, article in enumerate(articles[:3], 1):
                print(f"  {i}. {article['title'][:50]}...")
                print(f"     æƒ…ç·’: {article['sentiment']}")
        
        return True
    except Exception as e:
        print(f"âŒ Dcardçˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_mobile01_crawler():
    """æ¸¬è©¦Mobile01çˆ¬èŸ²"""
    print("\n=== æ¸¬è©¦Mobile01çˆ¬èŸ² ===")
    try:
        from crawler.mobile01_crawler import Mobile01Crawler
        
        crawler = Mobile01Crawler()
        
        # æ¸¬è©¦çˆ¬å–æ”¿æ²»è¨è«–ç‰ˆ
        keywords = ['æ”¿æ²»', 'é¸èˆ‰']
        articles = crawler.get_forum_articles('politics', 376, keywords, pages=1)
        
        print(f"âœ… Mobile01çˆ¬å–çµæœ: {len(articles)} ç¯‡æ–‡ç« ")
        
        if articles:
            print("å‰3ç¯‡æ–‡ç« :")
            for i, article in enumerate(articles[:3], 1):
                print(f"  {i}. {article['title'][:50]}...")
                print(f"     ä½œè€…: {article['author']}")
                print(f"     æƒ…ç·’: {article['sentiment']}")
        
        return True
    except Exception as e:
        print(f"âŒ Mobile01çˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_facebook_crawler():
    """æ¸¬è©¦Facebookçˆ¬èŸ²"""
    print("\n=== æ¸¬è©¦Facebookçˆ¬èŸ² ===")
    try:
        from crawler.fb_crawler import FacebookCrawler
        
        # ä¸æä¾›tokenï¼Œæ¸¬è©¦åˆå§‹åŒ–
        crawler = FacebookCrawler()
        
        print("âœ… Facebookçˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ (éœ€è¦access tokenæ‰èƒ½çˆ¬å–)")
        print("âš ï¸ è«‹è¨­ç½®Facebook access tokenä»¥æ¸¬è©¦å¯¦éš›çˆ¬å–åŠŸèƒ½")
        
        return True
    except Exception as e:
        print(f"âŒ Facebookçˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_integration():
    """æ¸¬è©¦æ•´åˆåŠŸèƒ½"""
    print("\n=== æ¸¬è©¦æ•´åˆåŠŸèƒ½ ===")
    try:
        from crawler.dcard_crawler import DcardCrawler
        from storage.sqlite_handler import SQLiteHandler
        from utils.common import statistics_calculator
        
        # å‰µå»ºçˆ¬èŸ²å’Œå„²å­˜
        crawler = DcardCrawler()
        storage = SQLiteHandler('integration_test.db')
        
        # çˆ¬å–å°‘é‡æ•¸æ“š
        keywords = ['æ”¿æ²»']
        articles = crawler.get_forum_articles('politics', keywords, pages=1)
        
        if articles:
            # å„²å­˜æ•¸æ“š
            result = storage.insert_articles(articles)
            print(f"âœ… æ•´åˆæ¸¬è©¦ - çˆ¬å–: {len(articles)} ç¯‡ï¼Œå„²å­˜: {result['inserted']} ç¯‡")
            
            # è¨ˆç®—çµ±è¨ˆ
            stats = statistics_calculator.calculate_sentiment_distribution(articles)
            print(f"âœ… æƒ…ç·’çµ±è¨ˆ: æ­£é¢ {stats['positive']}, è² é¢ {stats['negative']}, ä¸­æ€§ {stats['neutral']}")
        else:
            print("âš ï¸ æœªçˆ¬å–åˆ°æ•¸æ“šï¼Œä½†æ•´åˆæµç¨‹æ­£å¸¸")
        
        storage.close()
        return True
    except Exception as e:
        print(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
        return False

def generate_test_report():
    """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
    print("\n" + "="*60)
    print("ğŸ•·ï¸ æ¨¡çµ„åŒ–çˆ¬èŸ²ç³»çµ±æ¸¬è©¦å ±å‘Š")
    print("="*60)
    
    tests = [
        ("é…ç½®æ¨¡çµ„", test_config),
        ("å·¥å…·æ¨¡çµ„", test_utils),
        ("SQLiteå„²å­˜", test_sqlite_storage),
        ("Dcardçˆ¬èŸ²", test_dcard_crawler),
        ("Mobile01çˆ¬èŸ²", test_mobile01_crawler),
        ("Facebookçˆ¬èŸ²", test_facebook_crawler),
        ("æ•´åˆåŠŸèƒ½", test_integration)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ {test_name} æ¸¬è©¦ç•°å¸¸: {e}")
            results[test_name] = False
    
    # ç¸½çµ
    print("\n" + "="*60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµ")
    print("="*60)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name:15} : {status}")
    
    print(f"\nç¸½é«”çµæœ: {passed}/{total} é …æ¸¬è©¦é€šé ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼çˆ¬èŸ²ç³»çµ±æº–å‚™å°±ç·’")
    elif passed >= total * 0.7:
        print("âš ï¸ å¤§éƒ¨åˆ†æ¸¬è©¦é€šéï¼Œç³»çµ±åŸºæœ¬å¯ç”¨")
    else:
        print("âŒ å¤šé …æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç³»çµ±é…ç½®")
    
    # ä¿å­˜å ±å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_results': results,
        'summary': {
            'total_tests': total,
            'passed_tests': passed,
            'success_rate': passed / total,
            'status': 'ready' if passed == total else 'partial' if passed >= total * 0.7 else 'failed'
        }
    }
    
    with open('crawler_test_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: crawler_test_report.json")
    
    return report

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦æ¨¡çµ„åŒ–çˆ¬èŸ²ç³»çµ±...")
    report = generate_test_report()
    
    print(f"\nğŸ”— ç³»çµ±æ¶æ§‹èªªæ˜:")
    print(f"   ğŸ“ crawler/     - å„å¹³å°çˆ¬èŸ²æ¨¡çµ„")
    print(f"   ğŸ“ storage/     - æ•¸æ“šåº«å„²å­˜æ¨¡çµ„")
    print(f"   ğŸ“ utils/       - é€šç”¨å·¥å…·æ¨¡çµ„")
    print(f"   ğŸ“„ main_crawler.py - ä¸»æ§åˆ¶ç¨‹åº")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
    if report['summary']['status'] == 'ready':
        print(f"   âœ… ç³»çµ±å®Œå…¨å°±ç·’ï¼Œå¯ä»¥é–‹å§‹æ­£å¼çˆ¬å–")
        print(f"   ğŸš€ åŸ·è¡Œ: python main_crawler.py --keywords ç¾…æ™ºå¼· ç½·å… --pages 2")
    else:
        print(f"   âš ï¸ è«‹å…ˆè§£æ±ºå¤±æ•—çš„æ¸¬è©¦é …ç›®")
        print(f"   ğŸ”§ æª¢æŸ¥ç¶²çµ¡é€£æ¥å’Œä¾è³´å®‰è£")

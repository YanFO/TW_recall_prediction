#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å„ªåŒ–å™¨
æ”¶é›†æ­·å²ç½·å…é¸èˆ‰æ•¸æ“šï¼Œæ“´å¤§è¨“ç·´é›†ï¼Œå„ªåŒ–æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åƒæ•¸
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import json
from datetime import datetime
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelOptimizer:
    """æ¨¡å‹å„ªåŒ–å™¨"""
    
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
        # æ­·å²ç½·å…é¸èˆ‰æ•¸æ“š
        self.historical_data = self._create_historical_dataset()
        
    def _create_historical_dataset(self) -> pd.DataFrame:
        """å‰µå»ºæ­·å²ç½·å…é¸èˆ‰æ•¸æ“šé›†"""
        # å°ç£æ­·å²ç½·å…æ¡ˆä¾‹æ•¸æ“š (2018-2024)
        historical_cases = [
            # 2018å¹´é«˜é›„å¸‚é•·éŸ“åœ‹ç‘œç½·å…æ¡ˆ
            {
                'case_name': 'é«˜é›„å¸‚é•·éŸ“åœ‹ç‘œç½·å…æ¡ˆ',
                'year': 2020,
                'region': 'south',
                'position': 'mayor',
                'incumbent_party': 'KMT',
                'economic_satisfaction': 0.3,
                'political_satisfaction': 0.25,
                'media_coverage': 0.9,
                'social_media_sentiment': -0.4,
                'weather_impact': -0.1,
                'turnout_rate': 0.42,
                'support_rate': 0.97,
                'result': 'PASS'
            },
            # 2021å¹´å°ä¸­å¸‚ç¬¬äºŒé¸å€ç«‹å§”é™³æŸæƒŸç½·å…æ¡ˆ
            {
                'case_name': 'å°ä¸­å¸‚ç«‹å§”é™³æŸæƒŸç½·å…æ¡ˆ',
                'year': 2021,
                'region': 'central',
                'position': 'legislator',
                'incumbent_party': 'TPP',
                'economic_satisfaction': 0.6,
                'political_satisfaction': 0.4,
                'media_coverage': 0.7,
                'social_media_sentiment': -0.2,
                'weather_impact': 0.0,
                'turnout_rate': 0.52,
                'support_rate': 0.77,
                'result': 'PASS'
            },
            # 2022å¹´æ—æ˜¶ä½ç½·å…æ¡ˆ
            {
                'case_name': 'å°åŒ—å¸‚ç«‹å§”æ—æ˜¶ä½ç½·å…æ¡ˆ',
                'year': 2022,
                'region': 'north',
                'position': 'legislator',
                'incumbent_party': 'Independent',
                'economic_satisfaction': 0.5,
                'political_satisfaction': 0.45,
                'media_coverage': 0.6,
                'social_media_sentiment': 0.1,
                'weather_impact': -0.2,
                'turnout_rate': 0.42,
                'support_rate': 0.49,
                'result': 'FAIL'
            },
            # 2023å¹´ç‹æµ©å®‡ç½·å…æ¡ˆ
            {
                'case_name': 'æ¡ƒåœ’å¸‚è­°å“¡ç‹æµ©å®‡ç½·å…æ¡ˆ',
                'year': 2021,
                'region': 'north',
                'position': 'councilor',
                'incumbent_party': 'DPP',
                'economic_satisfaction': 0.55,
                'political_satisfaction': 0.4,
                'media_coverage': 0.8,
                'social_media_sentiment': -0.3,
                'weather_impact': 0.1,
                'turnout_rate': 0.51,
                'support_rate': 0.84,
                'result': 'PASS'
            },
            # æ¨¡æ“¬æ¡ˆä¾‹ - å¢åŠ æ•¸æ“šå¤šæ¨£æ€§
            {
                'case_name': 'æ¨¡æ“¬æ¡ˆä¾‹1',
                'year': 2023,
                'region': 'south',
                'position': 'mayor',
                'incumbent_party': 'DPP',
                'economic_satisfaction': 0.7,
                'political_satisfaction': 0.6,
                'media_coverage': 0.5,
                'social_media_sentiment': 0.2,
                'weather_impact': 0.0,
                'turnout_rate': 0.38,
                'support_rate': 0.35,
                'result': 'FAIL'
            },
            {
                'case_name': 'æ¨¡æ“¬æ¡ˆä¾‹2',
                'year': 2023,
                'region': 'central',
                'position': 'legislator',
                'incumbent_party': 'KMT',
                'economic_satisfaction': 0.4,
                'political_satisfaction': 0.3,
                'media_coverage': 0.8,
                'social_media_sentiment': -0.5,
                'weather_impact': -0.15,
                'turnout_rate': 0.55,
                'support_rate': 0.82,
                'result': 'PASS'
            }
        ]
        
        return pd.DataFrame(historical_cases)
    
    def prepare_features(self, df: pd.DataFrame) -> tuple:
        """æº–å‚™ç‰¹å¾µæ•¸æ“š"""
        # ç·¨ç¢¼åˆ†é¡è®Šé‡
        categorical_columns = ['region', 'position', 'incumbent_party']
        
        df_encoded = df.copy()
        
        for col in categorical_columns:
            if col in df_encoded.columns:
                df_encoded[f'{col}_encoded'] = self.label_encoder.fit_transform(df_encoded[col])
        
        # é¸æ“‡ç‰¹å¾µ
        feature_columns = [
            'economic_satisfaction', 'political_satisfaction', 'media_coverage',
            'social_media_sentiment', 'weather_impact', 'turnout_rate',
            'region_encoded', 'position_encoded', 'incumbent_party_encoded'
        ]
        
        available_features = [col for col in feature_columns if col in df_encoded.columns]
        
        X = df_encoded[available_features]
        y = df_encoded['result']
        
        return X, y
    
    def optimize_models(self) -> dict:
        """å„ªåŒ–å¤šå€‹æ¨¡å‹"""
        logger.info("é–‹å§‹æ¨¡å‹å„ªåŒ–...")
        
        X, y = self.prepare_features(self.historical_data)
        
        # åˆ†å‰²æ•¸æ“š
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # å®šç¾©æ¨¡å‹å’Œåƒæ•¸ç¶²æ ¼
        model_configs = {
            'RandomForest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            'GradientBoosting': {
                'model': GradientBoostingClassifier(random_state=42),
                'params': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7]
                }
            },
            'LogisticRegression': {
                'model': LogisticRegression(random_state=42),
                'params': {
                    'C': [0.1, 1, 10, 100],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                }
            },
            'SVM': {
                'model': SVC(random_state=42, probability=True),
                'params': {
                    'C': [0.1, 1, 10],
                    'kernel': ['linear', 'rbf'],
                    'gamma': ['scale', 'auto']
                }
            }
        }
        
        results = {}
        
        for model_name, config in model_configs.items():
            logger.info(f"å„ªåŒ– {model_name}...")
            
            try:
                # ç¶²æ ¼æœç´¢
                grid_search = GridSearchCV(
                    config['model'],
                    config['params'],
                    cv=3,  # ç”±æ–¼æ•¸æ“šè¼ƒå°‘ï¼Œä½¿ç”¨3æŠ˜äº¤å‰é©—è­‰
                    scoring='accuracy',
                    n_jobs=-1
                )
                
                # å°æ–¼éœ€è¦æ¨™æº–åŒ–çš„æ¨¡å‹ä½¿ç”¨æ¨™æº–åŒ–æ•¸æ“š
                if model_name in ['LogisticRegression', 'SVM']:
                    grid_search.fit(X_train_scaled, y_train)
                    y_pred = grid_search.predict(X_test_scaled)
                    y_pred_proba = grid_search.predict_proba(X_test_scaled)
                else:
                    grid_search.fit(X_train, y_train)
                    y_pred = grid_search.predict(X_test)
                    y_pred_proba = grid_search.predict_proba(X_test)
                
                # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
                accuracy = grid_search.score(X_test_scaled if model_name in ['LogisticRegression', 'SVM'] else X_test, y_test)
                
                # äº¤å‰é©—è­‰åˆ†æ•¸
                cv_scores = cross_val_score(
                    grid_search.best_estimator_,
                    X_train_scaled if model_name in ['LogisticRegression', 'SVM'] else X_train,
                    y_train,
                    cv=3
                )
                
                results[model_name] = {
                    'best_params': grid_search.best_params_,
                    'best_score': grid_search.best_score_,
                    'test_accuracy': accuracy,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'model': grid_search.best_estimator_
                }
                
                logger.info(f"{model_name} - æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.3f}, CVå¹³å‡: {cv_scores.mean():.3f}")
                
            except Exception as e:
                logger.error(f"å„ªåŒ– {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # é¸æ“‡æœ€ä½³æ¨¡å‹
        if results:
            best_model_name = max(results.keys(), key=lambda k: results[k]['test_accuracy'])
            self.best_model = results[best_model_name]['model']
            
            logger.info(f"æœ€ä½³æ¨¡å‹: {best_model_name} (æº–ç¢ºç‡: {results[best_model_name]['test_accuracy']:.3f})")
        
        self.models = results
        return results
    
    def save_optimized_model(self, filename: str = None) -> str:
        """ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹"""
        if not self.best_model:
            raise ValueError("å°šæœªè¨“ç·´æœ€ä½³æ¨¡å‹")
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"optimized_model_{timestamp}.joblib"
        
        # ä¿å­˜æ¨¡å‹å’Œé è™•ç†å™¨
        model_package = {
            'model': self.best_model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_names': list(self.historical_data.columns),
            'optimization_results': self.models
        }
        
        joblib.dump(model_package, filename)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {filename}")
        
        return filename
    
    def generate_synthetic_data(self, n_samples: int = 100) -> pd.DataFrame:
        """ç”Ÿæˆåˆæˆæ•¸æ“šä»¥æ“´å¤§è¨“ç·´é›†"""
        logger.info(f"ç”Ÿæˆ {n_samples} ç­†åˆæˆæ•¸æ“š...")
        
        synthetic_data = []
        
        for _ in range(n_samples):
            # åŸºæ–¼æ­·å²æ•¸æ“šçš„çµ±è¨ˆåˆ†å¸ƒç”Ÿæˆåˆæˆæ•¸æ“š
            case = {
                'case_name': f'åˆæˆæ¡ˆä¾‹_{_}',
                'year': np.random.choice([2020, 2021, 2022, 2023, 2024]),
                'region': np.random.choice(['north', 'central', 'south']),
                'position': np.random.choice(['mayor', 'legislator', 'councilor']),
                'incumbent_party': np.random.choice(['DPP', 'KMT', 'TPP', 'Independent']),
                'economic_satisfaction': np.random.normal(0.5, 0.15),
                'political_satisfaction': np.random.normal(0.45, 0.15),
                'media_coverage': np.random.beta(2, 2),
                'social_media_sentiment': np.random.normal(0, 0.3),
                'weather_impact': np.random.normal(0, 0.1),
                'turnout_rate': np.random.beta(2, 2) * 0.4 + 0.3,  # 30-70%
            }
            
            # åŸºæ–¼ç‰¹å¾µè¨ˆç®—çµæœæ¦‚ç‡
            success_prob = (
                (1 - case['economic_satisfaction']) * 0.3 +
                (1 - case['political_satisfaction']) * 0.3 +
                case['media_coverage'] * 0.2 +
                abs(case['social_media_sentiment']) * 0.1 +
                case['turnout_rate'] * 0.1
            )
            
            # æ·»åŠ éš¨æ©Ÿæ€§
            success_prob += np.random.normal(0, 0.1)
            success_prob = np.clip(success_prob, 0, 1)
            
            case['support_rate'] = success_prob
            case['result'] = 'PASS' if success_prob > 0.6 else 'FAIL'
            
            synthetic_data.append(case)
        
        return pd.DataFrame(synthetic_data)
    
    def train_with_augmented_data(self) -> dict:
        """ä½¿ç”¨æ“´å¢æ•¸æ“šè¨“ç·´æ¨¡å‹"""
        logger.info("ä½¿ç”¨æ“´å¢æ•¸æ“šè¨“ç·´æ¨¡å‹...")
        
        # ç”Ÿæˆåˆæˆæ•¸æ“š
        synthetic_df = self.generate_synthetic_data(200)
        
        # åˆä½µæ­·å²æ•¸æ“šå’Œåˆæˆæ•¸æ“š
        augmented_df = pd.concat([self.historical_data, synthetic_df], ignore_index=True)
        
        logger.info(f"æ“´å¢å¾Œæ•¸æ“šé›†å¤§å°: {len(augmented_df)} (åŸå§‹: {len(self.historical_data)}, åˆæˆ: {len(synthetic_df)})")
        
        # é‡æ–°è¨“ç·´æ¨¡å‹
        X, y = self.prepare_features(augmented_df)
        
        # ä½¿ç”¨æ›´å¤§çš„æ¸¬è©¦é›†æ¯”ä¾‹
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # è¨“ç·´æœ€ä½³æ¨¡å‹æ¶æ§‹
        best_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=7,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        
        best_model.fit(X_train, y_train)
        
        # è©•ä¼°æ€§èƒ½
        train_score = best_model.score(X_train, y_train)
        test_score = best_model.score(X_test, y_test)
        
        # äº¤å‰é©—è­‰
        cv_scores = cross_val_score(best_model, X, y, cv=5)
        
        results = {
            'train_accuracy': train_score,
            'test_accuracy': test_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'data_size': len(augmented_df),
            'feature_importance': dict(zip(X.columns, best_model.feature_importances_))
        }
        
        self.best_model = best_model
        
        logger.info(f"æ“´å¢æ•¸æ“šè¨“ç·´çµæœ - è¨“ç·´æº–ç¢ºç‡: {train_score:.3f}, æ¸¬è©¦æº–ç¢ºç‡: {test_score:.3f}")
        logger.info(f"äº¤å‰é©—è­‰: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")
        
        return results

def main():
    """ä¸»å‡½æ•¸"""
    optimizer = ModelOptimizer()
    
    print("ğŸ¤– é–‹å§‹æ¨¡å‹å„ªåŒ–...")
    
    # åŸºç¤æ¨¡å‹å„ªåŒ–
    print("\nğŸ“Š æ­¥é©Ÿ1: åŸºç¤æ¨¡å‹å„ªåŒ–")
    basic_results = optimizer.optimize_models()
    
    # æ“´å¢æ•¸æ“šè¨“ç·´
    print("\nğŸ“ˆ æ­¥é©Ÿ2: æ“´å¢æ•¸æ“šè¨“ç·´")
    augmented_results = optimizer.train_with_augmented_data()
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    print("\nğŸ’¾ æ­¥é©Ÿ3: ä¿å­˜å„ªåŒ–æ¨¡å‹")
    model_file = optimizer.save_optimized_model()
    
    # è¼¸å‡ºçµæœæ‘˜è¦
    print("\nğŸ¯ å„ªåŒ–çµæœæ‘˜è¦:")
    print(f"æœ€ä½³åŸºç¤æ¨¡å‹æº–ç¢ºç‡: {max([r['test_accuracy'] for r in basic_results.values()]):.3f}")
    print(f"æ“´å¢æ•¸æ“šæ¨¡å‹æº–ç¢ºç‡: {augmented_results['test_accuracy']:.3f}")
    print(f"äº¤å‰é©—è­‰åˆ†æ•¸: {augmented_results['cv_mean']:.3f} Â± {augmented_results['cv_std']:.3f}")
    print(f"è¨“ç·´æ•¸æ“šå¤§å°: {augmented_results['data_size']}")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_file}")
    
    # ä¿å­˜çµæœå ±å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"model_optimization_report_{timestamp}.json"
    
    report = {
        'basic_optimization': basic_results,
        'augmented_training': augmented_results,
        'model_file': model_file,
        'optimization_date': datetime.now().isoformat()
    }
    
    # åºåˆ—åŒ–æ¨¡å‹å°è±¡ç‚ºå­—ç¬¦ä¸²
    for model_name in basic_results:
        if 'model' in basic_results[model_name]:
            basic_results[model_name]['model'] = str(basic_results[model_name]['model'])
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‹ å„ªåŒ–å ±å‘Šå·²ä¿å­˜è‡³: {report_file}")

if __name__ == "__main__":
    main()

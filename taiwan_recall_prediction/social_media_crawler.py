#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šå¹³å°ç¤¾ç¾¤åª’é«”æ•¸æ“šæ”¶é›†å™¨
æ•´åˆFacebookã€Instagramã€Twitterã€YouTubeç­‰å¹³å°
"""

import os
import json
import time
import pandas as pd
from datetime import datetime, timedelta
import requests
from typing import List, Dict, Any
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SocialMediaCrawler:
    """å¤šå¹³å°ç¤¾ç¾¤åª’é«”çˆ¬èŸ²"""
    
    def __init__(self):
        self.recall_keywords = [
            'ç½·å…', 'ç½·éŸ“', 'ç½·ç‹', 'ç½·é™³', 'ç½·å…æŠ•ç¥¨', 'ä¸åŒæ„ç¥¨', 'åŒæ„ç¥¨',
            'ç½·å…æ¡ˆ', 'ç½·å…é€£ç½²', 'ç½·å…é–€æª»', 'ç½·å…æˆåŠŸ', 'ç½·å…å¤±æ•—',
            'ç«‹å§”ç½·å…', 'è­°å“¡ç½·å…', 'å¸‚é•·ç½·å…', 'ç¸£é•·ç½·å…'
        ]
        
        # APIé…ç½® (éœ€è¦åœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­ç½®)
        self.twitter_bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
        self.facebook_access_token = os.getenv('FACEBOOK_ACCESS_TOKEN')
        self.youtube_api_key = os.getenv('YOUTUBE_API_KEY')
        
        self.results = []
        
    def search_twitter(self, max_results: int = 100) -> List[Dict]:
        """æœç´¢Twitteræ•¸æ“š"""
        if not self.twitter_bearer_token:
            logger.warning("Twitter API token not found, skipping Twitter search")
            return []
            
        try:
            import tweepy
            
            # åˆå§‹åŒ–Twitter API
            client = tweepy.Client(bearer_token=self.twitter_bearer_token)
            
            tweets_data = []
            
            for keyword in self.recall_keywords[:3]:  # é™åˆ¶é—œéµå­—æ•¸é‡é¿å…APIé™åˆ¶
                try:
                    # æœç´¢æ¨æ–‡
                    tweets = client.search_recent_tweets(
                        query=f"{keyword} lang:zh",
                        max_results=min(max_results // len(self.recall_keywords[:3]), 100),
                        tweet_fields=['created_at', 'author_id', 'public_metrics', 'context_annotations']
                    )
                    
                    if tweets.data:
                        for tweet in tweets.data:
                            tweets_data.append({
                                'platform': 'Twitter',
                                'id': tweet.id,
                                'content': tweet.text,
                                'author_id': tweet.author_id,
                                'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                                'metrics': tweet.public_metrics if hasattr(tweet, 'public_metrics') else {},
                                'keyword': keyword,
                                'collected_at': datetime.now().isoformat()
                            })
                    
                    time.sleep(1)  # é¿å…APIé™åˆ¶
                    
                except Exception as e:
                    logger.error(f"Error searching Twitter for keyword '{keyword}': {e}")
                    continue
                    
            logger.info(f"Collected {len(tweets_data)} tweets")
            return tweets_data
            
        except ImportError:
            logger.error("tweepy not installed, skipping Twitter search")
            return []
        except Exception as e:
            logger.error(f"Twitter search failed: {e}")
            return []
    
    def search_youtube(self, max_results: int = 50) -> List[Dict]:
        """æœç´¢YouTubeæ•¸æ“š"""
        if not self.youtube_api_key:
            logger.warning("YouTube API key not found, skipping YouTube search")
            return []
            
        try:
            from googleapiclient.discovery import build
            
            youtube = build('youtube', 'v3', developerKey=self.youtube_api_key)
            
            videos_data = []
            
            for keyword in self.recall_keywords[:2]:  # é™åˆ¶é—œéµå­—æ•¸é‡
                try:
                    # æœç´¢è¦–é »
                    search_response = youtube.search().list(
                        q=keyword,
                        part='id,snippet',
                        maxResults=min(max_results // len(self.recall_keywords[:2]), 25),
                        type='video',
                        order='relevance',
                        regionCode='TW',
                        relevanceLanguage='zh'
                    ).execute()
                    
                    for item in search_response['items']:
                        video_id = item['id']['videoId']
                        snippet = item['snippet']
                        
                        # ç²å–è¦–é »çµ±è¨ˆæ•¸æ“š
                        stats_response = youtube.videos().list(
                            part='statistics',
                            id=video_id
                        ).execute()
                        
                        stats = stats_response['items'][0]['statistics'] if stats_response['items'] else {}
                        
                        videos_data.append({
                            'platform': 'YouTube',
                            'id': video_id,
                            'title': snippet.get('title', ''),
                            'description': snippet.get('description', ''),
                            'content': f"{snippet.get('title', '')} {snippet.get('description', '')}",
                            'channel': snippet.get('channelTitle', ''),
                            'published_at': snippet.get('publishedAt', ''),
                            'metrics': {
                                'viewCount': stats.get('viewCount', 0),
                                'likeCount': stats.get('likeCount', 0),
                                'commentCount': stats.get('commentCount', 0)
                            },
                            'keyword': keyword,
                            'collected_at': datetime.now().isoformat()
                        })
                    
                    time.sleep(1)  # é¿å…APIé™åˆ¶
                    
                except Exception as e:
                    logger.error(f"Error searching YouTube for keyword '{keyword}': {e}")
                    continue
                    
            logger.info(f"Collected {len(videos_data)} YouTube videos")
            return videos_data
            
        except ImportError:
            logger.error("google-api-python-client not installed, skipping YouTube search")
            return []
        except Exception as e:
            logger.error(f"YouTube search failed: {e}")
            return []
    
    def search_facebook_public(self, max_results: int = 50) -> List[Dict]:
        """æœç´¢Facebookå…¬é–‹å…§å®¹ (ä½¿ç”¨Graph API)"""
        if not self.facebook_access_token:
            logger.warning("Facebook access token not found, skipping Facebook search")
            return []
            
        try:
            facebook_data = []
            
            # æ³¨æ„ï¼šFacebook Graph APIå°æœç´¢æœ‰å¾ˆå¤šé™åˆ¶
            # é€™è£¡æä¾›åŸºæœ¬æ¡†æ¶ï¼Œå¯¦éš›ä½¿ç”¨éœ€è¦é©ç•¶çš„æ¬Šé™å’Œé…ç½®
            
            for keyword in self.recall_keywords[:2]:
                try:
                    # Facebook Graph APIæœç´¢ (éœ€è¦é©ç•¶æ¬Šé™)
                    url = f"https://graph.facebook.com/v18.0/search"
                    params = {
                        'q': keyword,
                        'type': 'post',
                        'access_token': self.facebook_access_token,
                        'limit': min(max_results // len(self.recall_keywords[:2]), 25)
                    }
                    
                    response = requests.get(url, params=params)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        for post in data.get('data', []):
                            facebook_data.append({
                                'platform': 'Facebook',
                                'id': post.get('id', ''),
                                'content': post.get('message', ''),
                                'created_time': post.get('created_time', ''),
                                'keyword': keyword,
                                'collected_at': datetime.now().isoformat()
                            })
                    
                    time.sleep(1)  # é¿å…APIé™åˆ¶
                    
                except Exception as e:
                    logger.error(f"Error searching Facebook for keyword '{keyword}': {e}")
                    continue
                    
            logger.info(f"Collected {len(facebook_data)} Facebook posts")
            return facebook_data
            
        except Exception as e:
            logger.error(f"Facebook search failed: {e}")
            return []
    
    def search_instagram_public(self, max_results: int = 50) -> List[Dict]:
        """æœç´¢Instagramå…¬é–‹å…§å®¹"""
        try:
            import instaloader
            
            L = instaloader.Instaloader()
            instagram_data = []
            
            for keyword in self.recall_keywords[:2]:
                try:
                    # æœç´¢hashtag
                    hashtag = instaloader.Hashtag.from_name(L.context, keyword.replace('#', ''))
                    
                    count = 0
                    for post in hashtag.get_posts():
                        if count >= max_results // len(self.recall_keywords[:2]):
                            break
                            
                        instagram_data.append({
                            'platform': 'Instagram',
                            'id': post.shortcode,
                            'content': post.caption if post.caption else '',
                            'author': post.owner_username,
                            'created_at': post.date.isoformat(),
                            'metrics': {
                                'likes': post.likes,
                                'comments': post.comments
                            },
                            'keyword': keyword,
                            'collected_at': datetime.now().isoformat()
                        })
                        
                        count += 1
                        time.sleep(2)  # é¿å…è¢«é™åˆ¶
                        
                except Exception as e:
                    logger.error(f"Error searching Instagram for keyword '{keyword}': {e}")
                    continue
                    
            logger.info(f"Collected {len(instagram_data)} Instagram posts")
            return instagram_data
            
        except ImportError:
            logger.error("instaloader not installed, skipping Instagram search")
            return []
        except Exception as e:
            logger.error(f"Instagram search failed: {e}")
            return []
    
    def collect_all_platforms(self, max_results_per_platform: int = 100) -> pd.DataFrame:
        """æ”¶é›†æ‰€æœ‰å¹³å°æ•¸æ“š"""
        logger.info("Starting multi-platform data collection...")
        
        all_data = []
        
        # æ”¶é›†å„å¹³å°æ•¸æ“š
        platforms = [
            ('Twitter', self.search_twitter),
            ('YouTube', self.search_youtube),
            ('Facebook', self.search_facebook_public),
            ('Instagram', self.search_instagram_public)
        ]
        
        for platform_name, search_func in platforms:
            logger.info(f"Collecting data from {platform_name}...")
            try:
                platform_data = search_func(max_results_per_platform)
                all_data.extend(platform_data)
                logger.info(f"Successfully collected {len(platform_data)} items from {platform_name}")
            except Exception as e:
                logger.error(f"Failed to collect data from {platform_name}: {e}")
        
        # è½‰æ›ç‚ºDataFrame
        if all_data:
            df = pd.DataFrame(all_data)
            
            # ä¿å­˜æ•¸æ“š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"social_media_data_{timestamp}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"Collected total {len(df)} items from all platforms")
            logger.info(f"Data saved to {filename}")
            
            # é¡¯ç¤ºå¹³å°åˆ†å¸ƒ
            platform_counts = df['platform'].value_counts()
            logger.info("Platform distribution:")
            for platform, count in platform_counts.items():
                logger.info(f"  {platform}: {count} items")
                
            return df
        else:
            logger.warning("No data collected from any platform")
            return pd.DataFrame()

def main():
    """ä¸»å‡½æ•¸"""
    crawler = SocialMediaCrawler()
    
    # æ”¶é›†æ•¸æ“š
    df = crawler.collect_all_platforms(max_results_per_platform=50)
    
    if not df.empty:
        print(f"\nâœ… æˆåŠŸæ”¶é›† {len(df)} ç­†ç¤¾ç¾¤åª’é«”æ•¸æ“š")
        print("\nğŸ“Š å¹³å°åˆ†å¸ƒ:")
        print(df['platform'].value_counts())
        
        print("\nğŸ“ æ•¸æ“šæ¨£æœ¬:")
        print(df[['platform', 'content']].head())
    else:
        print("âŒ æœªæ”¶é›†åˆ°ä»»ä½•æ•¸æ“š")

if __name__ == "__main__":
    main()
